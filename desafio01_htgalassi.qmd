---
title: "Desafio 01 - REDES NEURAIS ARTIFICIAIS (APRENDIZADO PROFUNDO)"
author: "Hanna Tatsuta Galassi"
date: "2025-09-17"
format:
  pdf:
    documentclass: article
    toc: true
    number-sections: true
    colorlinks: true
    geometry:
      - margin=1in
    echo: false
    warning: false
    standalone: true
---

```{python}
import warnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping

warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', message='.*retracing.*')
tf.keras.utils.disable_interactive_logging()
```

```{python}
path_data = "funcao.xlsx"
df = pd.read_excel(path_data)
```

```{python}
feature_cols = ["xgrd"]
target_col = ["ymean"]

df_features = df[feature_cols]
df_target = df[target_col]
```

```{python}
def train_and_plot_model(
    df_features,
    df_target,
    layer_nodes: list,
    activation_function: str,
    loss_function: str,
    optimizer_function: str,
    epochs: int
):
    """
    Constrói, treina e plota uma Rede Neural MLP para regressão.

    Args:
        df_features (pd.DataFrame): DataFrame com a(s) feature(s).
        df_target (pd.DataFrame): DataFrame com a variável target.
        layer_nodes (list): Lista com o número de neurônios por camada oculta.
        activation_function (str): Função de ativação para as camadas ocultas.
        loss_function (str): Função de perda para o modelo.
        optimizer_function (str): Otimizador para o treinamento.
        epochs (int): Quantidade de épocas para o treinamento.

    Returns:
        tf.keras.models.Sequential: O modelo Keras treinado.
    """
    # 1. Construção do modelo
    model_layers = []
    
    # Camada de entrada/primeira camada oculta
    model_layers.append(
        tf.keras.layers.Dense(
            layer_nodes[0], activation=activation_function,
            input_shape=(len(df_features.columns),))
    )
    
    # Camadas ocultas adicionais
    for nodes in layer_nodes[1:]:
        model_layers.append(tf.keras.layers.Dense(
            nodes, activation=activation_function))
        
    # Camada de saída (para regressão)
    model_layers.append(tf.keras.layers.Dense(1, activation="linear"))

    mlp_model = tf.keras.models.Sequential(model_layers)

    # 2. Compilação do modelo
    mlp_model.compile(optimizer=optimizer_function, loss=loss_function)

    # 3. Treinamento do modelo
    mlp_model.fit(df_features, df_target, epochs=epochs, verbose=0)

    # 4. Predição e plotagem
    X_range = np.linspace(
        df_features.min(), df_features.max(), 100).reshape(-1, 1)
    y_pred_curve = mlp_model.predict(X_range, verbose=0).flatten()

    plt.figure(figsize=(10, 7))
    plt.scatter(
        df_features, df_target, label='Dados Originais',
        color='blue', alpha=0.7)
    plt.plot(
        X_range, y_pred_curve, color='red', linewidth=3,
        label='Curva de Predição do Modelo')
    plt.title("Ajuste da Regressão com uma Única Feature")
    plt.xlabel("Feature (X)")
    plt.ylabel("Target (y)")
    plt.legend()
    plt.grid(True)
    plt.show()

    return mlp_model
```

# Introdução

Este relatório apresenta os resultados do desafio de modelagem de uma função univariada desconhecida usando uma Rede Neural Artificial do tipo Perceptron de Múltiplas Camadas (MLP). O objetivo principal do estudo é encontrar a topologia mínima (o número de camadas e neurônios) necessária para modelar a relação entre a feature xgrd e a variável alvo ymean.

Além de determinar a arquitetura ideal, o estudo investiga a influência de diferentes funções de ativação—ReLU, tangente hiperbólica (tanh) e sigmoide—no desempenho do modelo. Para garantir a robustez das conclusões, cada configuração testada foi executada múltiplas vezes para avaliar a variabilidade dos resultados. O relatório discute as principais descobertas obtidas a partir dessas análises, apresentando uma discussão sobre a eficiência e a capacidade de generalização de cada modelo.

# Discussão

O processo de desenvolvimento de um modelo de aprendizado profundo envolve a escolha de hiperparâmetros que influenciam diretamente seu desempenho. Esta seção detalha a metodologia utilizada para encontrar a topologia ideal da rede neural, bem como a avaliação do impacto de diferentes funções de ativação e da variabilidade inerente ao treinamento.

## Topologia mínima

A busca pela topologia mínima foi realizada por meio de uma abordagem iterativa. Após diversos testes, a topologia de uma única camada oculta com 8 neurônios e função de ativação sigmoide foi identificada como a arquitetura mais simples e eficiente para modelar a relação entre `xgrd` e `ymean` ao longo de 2000 passos. Essa arquitetura demonstrou ser suficiente para capturar a forma da função de maneira satisfatória, sem a necessidade de camadas adicionais ou maior complexidade.

A seguir, é apresentado um gráfico que ilustra o ajuste da topologia mínima encontrada (linha vermelha) em relação à curva entre `xgrd` e `ymean`.
```{python}
verbose = 0
activation_function = "sigmoid"
loss_function = "mse"
optimizer_function = "adam"

layer1_nodes = 8

epochs = 2000
```

```{python}
min_mlp = train_and_plot_model(
    df_features, df_target,
    layer_nodes=[layer1_nodes],
    activation_function=activation_function,
    loss_function=loss_function,
    optimizer_function=optimizer_function,
    epochs=epochs
)
```

## Utilização de diferentes funções de ativação
Com a topologia mínima definida, a análise foi expandida para avaliar a influência de três funções de ativação (ReLU, tangente hiperbólica (tanh) e sigmoide) no desempenho do modelo. Para cada função, foram treinados cinco modelos com as mesmas condições iniciais para analisar a variabilidade dos resultados. Os gráficos a seguir ilustram o ajuste dos cinco modelos treinados com cada função de ativação.
```{python}
min_topology_nodes = [layer1_nodes]

activation_functions = ["relu", "tanh", "sigmoid"]
num_models_per_config = 5

# Laço principal para testar cada função de ativação
for act_func in activation_functions:
    
    # Cria uma nova figura para cada função de ativação
    plt.figure(figsize=(12, 8))
    plt.scatter(
        df_features, df_target, label='Dados Originais', color='gray',
        alpha=0.5)
    
    # Laço para treinar múltiplos modelos e avaliar a variabilidade
    for i in range(num_models_per_config):
        
        # Construção do modelo com a topologia mínima
        mlp_model = tf.keras.models.Sequential(
            [
                tf.keras.layers.Dense(
                    min_topology_nodes[0], activation=act_func,
                    input_shape=(len(feature_cols),)),
                tf.keras.layers.Dense(1, activation="linear"),
            ]
        )
        mlp_model.compile(optimizer="adam", loss="mse")
        
        # Treinamento do modelo com o callback
        mlp_model.fit(
            df_features, df_target, epochs=epochs, verbose=0,
            )
        
        # Captura da curva de predição
        X_range_pred = np.linspace(
            df_features.min(), df_features.max(), 100).reshape(-1, 1)
        y_pred_curve = mlp_model.predict(X_range_pred, verbose=0).flatten()
        
        # Plotagem da curva no gráfico atual
        plt.plot(
            X_range_pred, y_pred_curve, linestyle='--',
            label=f'Modelo {i+1}', alpha=0.7, linewidth=2)

    # Configurações do gráfico
    plt.title(f"Ajuste com a Função de Ativação: {act_func.upper()}")
    plt.xlabel("Feature (xgrd)")
    plt.ylabel("Target (ymean)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
```

A topologia de 8 neurônios com a função de ativação ReLU demonstrou um comportamento consistente, mas resultou em um ajuste insatisfatório. Embora todos os cinco modelos tenham convergido para a mesma solução, a natureza linear da função ReLU para valores positivos de entrada impediu que a rede neural capturasse a forma completa dos dados originais. O resultado foi um modelo que se ajustou bem apenas à primeira parte da curva, falhando em modelar a segunda metade.

Em contraste, as funções sigmoide e tangente hiperbólica (tanh) demonstraram um desempenho superior e, principalmente, uma consistência notável. Em ambos os casos, todos os cinco modelos treinados convergiram para a mesma solução, ajustando-se de forma precisa e satisfatória aos dados originais. Essa consistência, evidenciada pela sobreposição das curvas, comprova que a topologia de 8 neurônios, combinada com as funções sigmoide ou tanh, é robusta e capaz de encontrar a solução ideal de forma confiável para este problema.


# Conclusões
O estudo demonstrou que a topologia de uma única camada oculta com 8 neurônios foi a arquitetura mínima capaz de capturar a relação complexa entre as variáveis.

A análise da influência de diferentes funções de ativação mostrou que a escolha desse hiperparâmetro é crítica para o desempenho do modelo, mas que, quando bem-escolhida, a topologia é robusta. A função ReLU, apesar de popular, se mostrou inadequada para essa tarefa, falhando em modelar completamente a relação entre as variáveis. Por outro lado, as funções sigmoide e tangente hiperbólica (tanh) não apenas se ajustaram com perfeição aos dados, mas também garantiram que o treinamento fosse consistente, com múltiplos modelos convergindo para a mesma solução ideal.

A principal conclusão é que, para um desempenho consistente e confiável, a topologia de 8 neurônios combinada com uma função de ativação como a tanh ou a sigmoide é a escolha mais robusta. O experimento demonstrou que essas funções, ao contrário da ReLU, conseguem capturar a complexidade da curva e oferecem a confiabilidade necessária para que o modelo encontre a melhor solução em diferentes rodadas de treinamento